<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Ching-Yao Chuang</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Homepage and blog">
    <meta name="author" content="Ching-Yao Chuang">

    <link rel="canonical" href="https://JamesChuanggg.github.io/blog/">
    <link rel="alternate" type="application/rss+xml" title="RSS Feed for Avi Singh" href="/feed.xml" />

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/pixyll.css?201602132020" type="text/css">

    <!-- Fonts -->
    <link href='//fonts.googleapis.com/css?family=Merriweather:900,900italic,300,300italic' rel='stylesheet' type='text/css'>
    <link href='//fonts.googleapis.com/css?family=Lato:900,300' rel='stylesheet' type='text/css'>

      <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css" rel="stylesheet">


    <!-- Verifications -->


    <!-- Open Graph -->
    <!-- From: https://github.com/mmistakes/hpstr-jekyll-theme/blob/master/_includes/head.html -->
    <meta property="og:locale" content="en_US">
    <meta property="og:type" content="article">
    <meta property="og:title" content="Blog">
    <meta property="og:description" content="Homepage and blog">
    <meta property="og:url" content="https://JamesChuanggg.github.io/blog/">
    <meta property="og:site_name" content="Yuan-Hong Liao">

    <!-- Twitter Card -->
    <meta name="twitter:card" content="summary" />

        <meta name="twitter:site" content="@andrewliao11" />

    <meta name="twitter:title" content="Blog" />
    <meta name="twitter:description" content="Homepage and blog" />
    <meta name="twitter:url" content="https://JamesChuanggg.github.io/blog/" />

    <!-- Icons -->
    <link rel="apple-touch-icon" sizes="57x57" href="/apple-icon-57x57.png">
    <link rel="apple-touch-icon" sizes="114x114" href="/apple-icon-114x114.png">
    <link rel="apple-touch-icon" sizes="72x72" href="/apple-icon-72x72.png">
    <link rel="apple-touch-icon" sizes="144x144" href="/apple-icon-144x144.png">
    <link rel="apple-touch-icon" sizes="60x60" href="/apple-icon-60x60.png">
    <link rel="apple-touch-icon" sizes="120x120" href="/apple-icon-120x120.png">
    <link rel="apple-touch-icon" sizes="76x76" href="/apple-icon-76x76.png">
    <link rel="apple-touch-icon" sizes="152x152" href="/apple-icon-152x152.png">
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-icon-180x180.png">
    <link rel="icon" type="image/png" href="/favicon-160x160.png" sizes="160x160">
    <link rel="icon" type="image/png" href="/favicon-96x96.png" sizes="96x96">
    <link rel="icon" type="image/png" href="/favicon-16x16.png" sizes="16x16">
    <link rel="icon" type="image/png" href="/favicon-32x32.png" sizes="32x32">


    <script type="text/javascript">
      var _gaq = _gaq || [];
      _gaq.push(['_setAccount', 'UA-63278661-1']);
      _gaq.push(['_trackPageview']);
      (function() {
        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
      })();
    </script>

</head>

<body class="site">
  <div class="site-wrap">
    <header class="site-header px2 px-responsive">
  <div class="mt2 wrap">
    <div class="measure">
      <a href="https://JamesChuanggg.github.io" class="site-title"></a>
      <nav class="site-nav">
        <a href="/">Home</a>
<a href="/blog/">Blog</a>
      </nav>
      <div class="clearfix" ></div>

        <div class="social-icons">
  <div class="left">

      <a class="fa fa-github" href="https://github.com/JamesChuanggg"></a>


      <a class="fa fa-twitter" href="https://twitter.com/deep_jamesc"></a>


      <a class="fa fa-linkedin" href="https://tw.linkedin.com/in/ching-yao-chuang-452374125"></a>



  </div>
  <div class="right">




  </div>
</div>

<div class="clearfix">

  <center> <a class="fa fa-envelope" href="mailto:james874286@gmail.com"></a> james847286@gmail.com <center>

</div>




    </div>
  </div>
</header>


    <div class="post p2 p-responsive wrap" role="main">
      <div class="measure">



<div class="post-header mb2">
  <h1>AlphaGo: gotta beat 'em all<Down></h1>
  <span class="post-meta">Aug 19, 2016</span><br>

  <span class="post-meta small">

    paper from Google DeepMind

  </span>
</div>

<article class="post-content">
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<p><img src="/blogpost/images/alphago/logo.png" alt="Teaser" /></p>


<p>In March 2016, <a href="https://deepmind.com/alpha-go">AlphaGo</a> beat Lee Sedol in a five-game match which aroused lots of people's interest in Artificial Intelligence.
So what's the difference between the traditional algorithm and AlphaGo?
Today we will take a deep look into this work: <a href="http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html">Mastering the game of Go with deep neural networks and tree search</a>,
published in Nature.</p>

<p>Go is a extremely large game, the breadth and depth of the search tree are about 250 and 150, so exhaustive search is infeasible.
Thanks to Deep Learning, we can now use deep neural network to truncate the tree.
<br>There are three main structures in AlphaGo,</p>
<ol>
  <li>Policy Network (SL, Rollout, RL)</li>
  <li>Value Network</li>
  <li>Monte Carlo Tree Search (MCTS)</li>
</ol>

<h2>Policy Network</h2>
<p>Policy network, p(a|s), take a state s as input and output the probability distribution over possible moves a.
We begin by training a supervised learning (SL) policy network directly from expert human moves.
We also train a fast policy that can rapidly sample actions during rollouts. Next, we train a reinforcement learning (RL)
policy network that improves the SL policy network by optimizing the final outcome of games of self-play.</p>

<h4>Supervised Learning Policy Network</h4>
<p>The SL policy network is trained on randomly sampled state-action pairs (s, a) from 30 million positions from the KGS Go Server.
The goal of supervised learning policy network is to predict expert moves, using stochastic gradient ascent to maximize the likelihood of the human move a selected in state s </p>

<center><p><img src="/blogpost/images/alphago/sl_p.png" alt="Teaser" /></p></center>

<h4>Rollout Policy Network</h4>
<p>Here we also supervisely trained (same way as SL policy network) a faster but less accurate policy network using a linear softmax of small pattern features.
Why don't we use SL policy network directly? Unfortunately, it's impossible to play to the end by using DEEP neural network during tree search.
So we build a smaller network forming by local pattern matching and logistic regression which has wildly been used in advertising recommendation, news sorting, etc.
Be ware that the whole rollout network has "no deep learning architecture", which means that it need no GPU!
It use just 2 μs to select an action, rather than 3 ms for the SL policy network.</p>

<h4>Reinforcement Learning Policy Networks</h4>
<p>Now we aim at improving the policy network by reinforcement learning (RL). Its weights are initialized to the same value as SL policy network,
then play with randomly selected previous iteration of the policy network. Reward r(s) is zero for all non-terminal step. The outcome z = ± r(st) is the terminal reward,
+1 for winning and −1 for losing. Its Weights are then updated in the direction that maximizes expected outcome</p>

<center><p><img src="/blogpost/images/alphago/rl_p.png" alt="Teaser" /></p></center>

<p>One thing worthed to mentioned is that we use SL policy network while searching in tree instead of
RL policy network since the output of RL policy is less variant then SL policy network. Humans select a diverse beam of promising moves, whereas RL
optimizes for the single best move. However we use RL policy network
for training value network cause it can do better on outcome optimizing due to reinforcement learning. </p>

<h2>Value Network</h2>
<p>A value network v(s) predicts the outcome from position s of games. To avoid the correlation of successive positions,
we generated a new self-play data set consisting of 30 million distinct positions, each sampled from a separate game played between the RL policy network.
The network use using stochastic gradient descent to minimize the mean squared error between the predicted value vθ(s), and the corresponding outcome z.
</p>

<center><p><img src="/blogpost/images/alphago/v.png" alt="Teaser" /></p></center>

<p>Here's the overall training pipeline and network structure from figure 1 of the paper. We can see that the output of a policy network has the same size as the board
while the output of value network is a single value. Though we have powerful networks now, but we still need a algorithm to assemble
them.</p>

<center><p><img src="/blogpost/images/alphago/figure1.png" alt="Teaser" /></p></center>

<h2>Monte Carlo Tree Search</h2>
<p>Here we combine policy network(SL and Rollout) and value network to select moves. Before digging into alphago's algorithm,
let's talk about the concept of <a href="https://en.wikipedia.org/wiki/Monte_Carlo_tree_search">Monte Carlo tree search</a> (MCTS). Monte Carlo tree search is on the analysis of the most promising moves,
expanding the search tree based on random sampling of the search space.
<br>Each round of simulation contains four steps, (from wikipedia)</p>
<ol>
  <li>Selection: start from root node R and then select successive child nodes down to a leaf node L</li>
  <li>Expansion: create one or more child nodes or choose from them node C if leaf node L is not the end of game</li>
  <li>Simulation: play a random playout from node C</li>
  <li>Backpropagation: use the result of the playout to update information in the nodes on the path from C to R</li>
</ol>

<center><p><img src="/blogpost/images/alphago/mcts.png" alt="Teaser" /></p></center>

<h4>Selection</h4>
<p>In AlphaGo, each edge (s, a) stores three value: action value Q(s, a), visit count N(s, a), and prior probability P(s, a).
We can imagine the prior probability P as a placeholder for the output probability from SL policy network since it's source wasted
to compute them each round, so we use a placeholder to store the output. At each time step t of each simulation, an action a
is selected from state s based on three values aboved.</p>

<center><p><img src="/blogpost/images/alphago/selection1.png" alt="Teaser" /></p></center>

<p>so as to maximize action value plus a bonus</p>

<center><p><img src="/blogpost/images/alphago/selection2.png" alt="Teaser" /></p></center>

<p>We can regard Q as "exploit" and u as "exploration". However u is not purely for exploration since prior probabilities P
will help us choosing move like a human expert due to SL policy network while visit count N make us explore the other moves.
Here policy network help us sample action without computing all the branch. <img src="/blogpost/images/alphago/expansion.png" style="float: right; margin: 15px" alt="Teaser" /></p>

<h4>Expansion</h4>
<p>When the traversal reaches a leaf node at step L (visit count exceeds a threshold), the leaf node may be expanded
(new node is initialized). The leaf position is processed just once by the SL policy network.
The output probabilities are stored as prior probabilities P for each legal action a (put them into placeholder).</p>

<h4>Evaluation</h4>
<p>We combine the output from value network and the outcome z of a random rollout played out until terminal step T using the fast rollout policy
using paramater λ. Here rollout policy network show his power of computing speed. Value network can help us evaluate the node more precisely.
</p>

<center><p><img src="/blogpost/images/alphago/evaluation.png" alt="Teaser" /></p></center>

<h4>Backup</h4>
<p>The last thing to do is updating the edge we traversed before. Each edge accumulates the visit count and mean evaluation
of all simulations passing through that edge.
</p>

<center><p><img src="/blogpost/images/alphago/backup.png" alt="Teaser" /></p></center>

<h2>Conclusion</h2>
<p><img src="/blogpost/images/alphago/nature.png" style="float: right; margin: 15px" alt="Teaser" />
Actually Monte Carlo tree search is not new stuff, the thing that really maters is "deep neural network".
Deep learning has achieved excellent performance in many different field like computer vision, nature language process, etc.
Another <a href="http://www.nature.com/nature/journal/v518/n7540/full/nature14236.html">paper</a> DeepMind publish on Nature
useing deep Q network can also show the power of deep neural network. In AlphaGo, the depth of the search is reduced by position evaluation
and the breadth of the search may be reduced by sampling actions from a policy p(a|s). These networks make searching the tree of Go possible.
There are more details in the paper, for example, how they implement such a complex system. Anyway, AlphaGo is super awesome, good job DeepMind!
</p>

<h2>Reference</h2>
<p>David S et al, <a href="http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html">Mastering the game of Go with deep neural networks and tree search</a>
Wikipedia, <a href="https://en.wikipedia.org/wiki/Monte_Carlo_tree_search">Monte Carlo tree search</a>
</p>

<!--
<p>In the last couple of years, a number of papers (like <a href="http://www.pnas.org/content/112/12/3618.abstract">this paper from JHU/Brown</a>, and <a href="http://arxiv.org/abs/1410.8027">this one from MPI</a>) have suggested that the task of Visual Question Answering (VQA, for short) can be used as an alternative Turing Test. The task involves answering an open-ended question (or a series of questions) about an image. An example is shown below:</p>

<h5 id="image-from-visualqaorg">Image from visualqa.org</h5>
<p><img src="/images/vqa/challenge.png" alt="Visual QA" /></p>

<p>The AI system needs to solve a number of sub-problems in Natural Language Processing and Computer Vision, in addition to being able to perform some kind of “common-sense” reasoning. It needs to localize the subject being referenced (the woman’s face, and more specifically the region around her lips), needs to detect objects (the banana), and should also have some common-sense knowledge that the word <em>mustache</em> is often used to refer to markings or objects on the face that are not actually mustaches (like milk mustaches). Since the problem cuts through two two very different modalities (vision and text), and requires high-level understanding of the scene, it appears to be an ideal candidate for a true Turing Test. The problem also has real world applications, like helping the <a href="https://itunes.apple.com/us/app/vizwiz/id439686043?mt=8">visually impaired</a>.</p>

<p>A few days ago, the <a href="http://visualqa.org/challenge.html">Visual QA Challenge</a> was launched, and along with it came a large dataset (~750K questions on ~250K images). After the <a href="http://mscoco.org/dataset/#captions-challenge2015">MS COCO Image Captioning Challenge</a> sparked a lot of interest in problem of <a href="https://pdollar.wordpress.com/2015/01/21/image-captioning/">image captioning</a> (or was it the interest that led to the challenge?), the time seems ripe to move onto a much harder problem at the intersection of NLP and Vision.</p>

<p>This post will present ways to model this problem using Neural Networks, exploring both Feedforward Neural Networks, and the much more exciting <strong>Recurrent Neural Networks</strong> (LSTMs, to be specific). If you do not know much about Neural Networks, then I encourage you to check these two awesome blogs: <a href="https://colah.github.io">Colah’s Blog</a> and <a href="https://karpathy.github.io">Karpathy’s Blog</a>. Specifically, check out the posts on Recurrent Neural Nets, Convolutional Neural Nets and LSTM Nets. The models in this post take inspiration from <a href="https://filebox.ece.vt.edu/~parikh/Publications/ICCV2015_VQA.pdf">this ICCV 2015 paper</a>, <a href="https://www.d2.mpi-inf.mpg.de/sites/default/files/iccv15-neural_qa.pdf">this ICCV 2015 paper</a>, and <a href="http://www.cs.toronto.edu/~mren/imageqa/">this NIPS 2015 paper</a>.</p>

<h2 id="generating-answers">Generating Answers</h2>
<p>An important aspect of solving this problem is to have a system that can generate new answers. While most of the answers in the VQA dataset are short (1-3 words), we would still like to a have a system that can generate arbitrarily long answers, keeping up with our spirit of the Turing test. We can perhaps take inspiration from papers on <a href="http://arxiv.org/abs/1409.3215">Sequence to Sequence Learning using RNNs</a>, that solve a similar problem when generating translations of arbitrary length. <a href="http://papers.nips.cc/paper/5411-a-multi-world-approach-to-question-answering-about-real-world-scenes-based-on-uncertain-input.pdf">Multi-word methods</a> have been presented for VQA too. However, for the purpose of this blog post, we will ignore this aspect of the problem. We will select the 1000 most frequent answers in the VQA training dataset, and solve the problem in a multi-class classification setting. These top 1000 answers cover over 80% of the answers in the VQA training set, so we can still expect to get reasonable results.</p>

<h2 id="the-feedforward-neural-model">The Feedforward Neural Model</h2>
<p><img src="/images/vqa/model_1.jpg" alt="The MLP Model" /></p>

<p>To get started, let’s first try to model the problem using a <a href="https://en.wikipedia.org/wiki/Multilayer_perceptron">MultiLayer Perceptron</a>. An MLP is a simple feedforward neural net that maps a feature vector (of fixed length) to an appropriate output. In our problem, this output will be a probability distribution over the set of possible answers. We will be using <a href="http://keras.io">Keras</a>, an awesome deep learning library based on <a href="http://deeplearning.net/software/theano/">Theano</a>, and written in Python. Setting up Keras is fairly easy, just have a look at their <a href="https://github.com/fchollet/keras#installation">readme</a> to get started.</p>

<p>In order to use the MLP model, we need to map all our input questions and images to a feature vector of fixed length. We perform the following operations to achieve this:</p>

<ol>
  <li>For the question, we transform each word to its <a href="https://code.google.com/p/word2vec/">word vector</a>, and sum up all the vectors. The length of this feature vector will be same as the length of a single word vector, and the word vectors (also called embeddings) that we use have a length of <code>300</code>.</li>
  <li>For the image, we pass it through a Deep Convolutional Neural Network (the well-known <a href="http://arxiv.org/abs/1409.1556">VGG Architecture</a>), and extract the activation from the second last layer (before the softmax layer, that is). Size of this feature vector is <code>4096</code>.</li>
</ol>

<p>Once we have generated the feature vectors, all we need to do now is to define a model in Keras, set up a cost function and an optimizer, and we’re good to go.
The following Keras code defines a multi-layer perceptron with two hidden layers, <code>1024</code> hidden units in each layer and dropout layers in the middle for regularization. The final layer is a softmax layer, and is responsible for generating the probability distribution over the set of possible answers. I have used the <code>categorical_crossentropy</code> loss function since it is a multi-class classification problem. The <code>rmsprop</code> method is used for optimzation. You can try experimenting with other optimizers, and see what kind of <a href="http://lossfunctions.tumblr.com/">learning curves</a> you get.</p>

<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">keras.layers.core</span> <span class="kn">import</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Dropout</span><span class="p">,</span> <span class="n">Activation</span>

<span class="n">img_dim</span> <span class="o">=</span> <span class="mi">4096</span> <span class="c">#top layer of the VGG net</span>
<span class="n">word_vec_dim</span> <span class="o">=</span> <span class="mi">300</span> <span class="c">#dimension of pre-trained word vectors</span>
<span class="n">nb_hidden_units</span> <span class="o">=</span> <span class="mi">1024</span> <span class="c">#number of hidden units, a hyperparameter</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">nb_hidden_units</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="n">img_dim</span><span class="o">+</span><span class="n">word_vec_dim</span><span class="p">,</span>
          <span class="n">init</span><span class="o">=</span><span class="s">&#39;uniform&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s">&#39;tanh&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">nb_hidden_units</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s">&#39;uniform&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s">&#39;tanh&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">nb_classes</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s">&#39;uniform&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s">&#39;softmax&#39;</span><span class="p">))</span>

<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">&#39;categorical_crossentropy&#39;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s">&#39;rmsprop&#39;</span><span class="p">)</span></code></pre></div>

<p>Have a look at the <a href="https://github.com/avisingh599/visual-qa/blob/master/scripts/trainMLP.py">entire python script</a> to see the code for generating the features and training the network. It does not access the hard disk once the training begins, and uses about ~4GB of RAM. You can reduce memory usage by lowering the <code>batchSize</code> variable, but that would also lead to longer training times. It is able to process over 215K image-question pairs in less than <strong>160 seconds/epoch</strong> when working on a GTX 760 GPU with a batch size of 128. I ran my experiments for 100 epochs.</p>

<h2 id="the-recurrent-neural-model">The Recurrent Neural Model</h2>
<p><img src="/images/vqa/lstm_encoder.jpg" alt="The LSTM Model" align="middle" style="width: 500px;" /></p>

<p>A drawback of the previous approach is that we ignore the sequential nature of the questions. Regardless of what order the words appear in, we’ll get the same vector representing the question, à la <a href="https://en.wikipedia.org/wiki/Bag-of-words_model">bag-of-words (BOW)</a>. A way to tackle this limitation is by use of <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">Recurrent Neural Networks</a>, which are well-suited for sequential data. We’ll be using <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">LSTMs</a> here, since they avoid some common nuances of vanilla RNNs, and often give a slightly better performance. You can also experiment with other recurrent layers in Keras, such as <code>GRU</code>. The word vectors corresponding to the tokens in the question are passed to an LSTM in a sequential fashion, and the output of the LSTM (from its output gate) after all the tokens have been passed is chosen as the representation for the entire question. This fixed length vector is concatenated with the <code>4096</code> dimensional CNN vector for the image, and passed on to a multi-layer perceptron with fully connected layers. The last layer is once again softmax, and provides us with a probability distribution over the possible outputs.</p>

<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">keras.layers.core</span> <span class="kn">import</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Activation</span><span class="p">,</span> <span class="n">Merge</span><span class="p">,</span> <span class="n">Dropout</span><span class="p">,</span> <span class="n">Reshape</span>
<span class="kn">from</span> <span class="nn">keras.layers.recurrent</span> <span class="kn">import</span> <span class="n">LSTM</span>

<span class="n">num_hidden_units_mlp</span> <span class="o">=</span> <span class="mi">1024</span>
<span class="n">num_hidden_units_lstm</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">img_dim</span> <span class="o">=</span> <span class="mi">4096</span>
<span class="n">word_vec_dim</span> <span class="o">=</span> <span class="mi">300</span>

<span class="n">image_model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">image_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Reshape</span><span class="p">(</span><span class="n">input_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">img_dim</span><span class="p">,),</span> <span class="n">dims</span><span class="o">=</span><span class="p">(</span><span class="n">img_dim</span><span class="p">,)))</span>

<span class="n">language_model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">language_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="n">output_dim</span> <span class="o">=</span> <span class="n">num_hidden_units_lstm</span><span class="p">,</span>
			<span class="n">return_sequences</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
			<span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="n">max_len</span><span class="p">,</span> <span class="n">word_vec_dim</span><span class="p">)))</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Merge</span><span class="p">([</span><span class="n">language_model</span><span class="p">,</span> <span class="n">image_model</span><span class="p">],</span>
			<span class="n">mode</span><span class="o">=</span><span class="s">&#39;concat&#39;</span><span class="p">,</span> <span class="n">concat_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">num_hidden_units_mlp</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s">&#39;uniform&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s">&#39;tanh&#39;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">num_hidden_units_mlp</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s">&#39;uniform&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s">&#39;tanh&#39;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">nb_classes</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s">&#39;softmax&#39;</span><span class="p">))</span>

<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">&#39;categorical_crossentropy&#39;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s">&#39;rmsprop&#39;</span><span class="p">)</span></code></pre></div>

<p>A single <code>train_on_batch</code> method call in Keras expects the sequences to be of the same length (so that is can be represented as a Theano Tensor). There has been a lot of discussion regarding training LSTMs with variable length sequences, and I used the following technique: Sorted all the questions by their length, and then processed them in batches of <code>128</code> while training. Most batches had questions of the same length (say 9 or 10 words), and there was no need of zero-padding. For the few batched that did have questions of varying length, the shorter questions were zero-padded. I was able to achieve a training speed of <strong>200 seconds/epoch</strong> on a GTX 760 GPU.</p>

<h2 id="show-me-the-numbers">Show me the numbers</h2>
<p>I trained my system on the Training Set of the VQA dataset, and evaluated performance on the validation set, following the rules of the VQA challenge. The answer produced by the Neural Net is checked against every answer provided by humans (there are ten human answers for every question). If the answer produced by the neural net <em>exactly</em> matches <em>at least</em> three of the ten answers, then we classify it as a correct prediction. Here is the performance of the models that I trained:</p>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th style="text-align: center">Accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>BOW+CNN</td>
      <td style="text-align: center">48.46%</td>
    </tr>
    <tr>
      <td>LSTM-Language only</td>
      <td style="text-align: center">44.17%</td>
    </tr>
    <tr>
      <td>LSTM+CNN</td>
      <td style="text-align: center">51.63%</td>
    </tr>
  </tbody>
</table>

<p><strong>Update</strong>: The results that I reported earlier were based on a metric slightly different from the ones used on VQA. They have since been updated. Also, I was able to obtain a performance of <strong>53.34%</strong> on the test-dev set (LSTM+CNN), which is practically the same as those set by the VQA authors in their LSTM baseline.</p>

<p>It’s interesting to see that even a “blind” model is able to obtain an accuracy of 44.17%. This shows that the model is pretty good at guessing the answers once it has identified the type of question. The LSTM+CNN model shows an improvement of about 3% as compared to the Feedforward Model (BOW+CNN), which tells us that the temporal structure of the question is indeed helpful. These results are in line with what was obtained in the <a href="http://www.visualqa.org/VQA_ICCV2015.pdf">original VQA paper</a>. However, the results reported in the paper were on the <em>test</em> set (trained on train+val), while we have evaluated on the <em>validation</em> set (trained on only train). If we learn a model on both the training and the validation data, then we can expect a significant improvement in performance since the number of training examples will increase by 50%. Finally, there is a lot of scope for hyperparameter tuning (number of hidden units, number of MLP hidden layers, number of LSTM layers, dropout or no dropout etc.).</p>

<p>I carried out my experiments for 100 epochs<sup id="fnref:1"><a href="#fn:1" class="footnote">1</a></sup>, and observed the following curve:</p>

<p><img src="/images/vqa/learning_curve.jpg" alt="Validation Accuracy with number of epochs" /></p>

<p>The LSTM+CNN model flattens out in performance after about 50 epochs. The BOW+CNN also showed similar behavior, but took a surprising dive at epoch 90, which was soon rectified by the 100th epoch. I’ll probably re-initialize and run the models for 500 epochs, and see if such behavior is seen again or not. <strong>Update</strong>: I did run it once more, and the dip was not observed!</p>

<h3 id="a-note-on-word-embeddings">A note on word embeddings</h3>
<p>We have a number of choices when using word embeddings, and I experimented with three of them:</p>

<ol>
  <li>
    <p><a href="http://nlp.stanford.edu/projects/glove/">GloVe Word Embeddings</a> trained on the common-crawl: These gave the best performance, and all results reported here are using these embeddings.</p>
  </li>
  <li>
    <p><a href="https://levyomer.wordpress.com/2014/04/25/dependency-based-word-embeddings/">Goldberg and Levy 2014</a>: These are the default embeddings that come with <a href="http://spacy.io/">spaCy</a>, and they gave significantly worse results.</p>
  </li>
  <li>
    <p>Embeddings Trained on the VQA questions: I used <a href="https://radimrehurek.com/gensim/models/word2vec.html">Gensim’s word2vec</a> implementation to train my own embeddings on the questions in the training set of the VQA dataset. The performance was similar to, but slighly worse than the GloVe embeddings. This is primarily because the VQA training set alone is not sufficiently large (~2.5m words) to get reasonable word vectors, especially for less common words.</p>
  </li>
</ol>

<h3 id="link-to-github-repohttpsgithubcomavisingh599visual-qa"><a href="https://github.com/avisingh599/visual-qa">Link to github repo</a></h3>

<hr />
<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>Validation was done once per 10 epochs for BOW+CNN, once every 5 epochs for LSTMs. <a href="#fnref:1" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
-->
</article>








  <div id="disqus_thread"></div>
  <script type="text/javascript">
    var disqus_shortname  = 'alphago';
    var disqus_identifier = '/blogpost/alphago';
    var disqus_title      = '';

    (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>




      </div>
    </div>
  </div>

  <footer class="center">
  <div class="measure">
    <small>
      Powered by <a href="https://jekyllrb.com/">Jekyll</a> using <a href="https://pixyll.com/">Pixyll</a> theme. Hosted on <a href="https://pages.github.com/">Github Pages</a>. <br>
      © James Chuang
    </small>
  </div>
</footer>

</body>
</html>
